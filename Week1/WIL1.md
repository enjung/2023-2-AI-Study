## WIL1
# Deep Learning Basic

2023.09.13 1ì£¼ì°¨

## AI, ML, DLì˜ ì°¨ì´ì™€ ê´€ê³„

AI(Artificial Inteligence) : ì¸ê°„ì˜ ëŠ¥ë ¥ì„ ì¸ê³µì ìœ¼ë¡œ êµ¬í˜„

ML(Machine Learning) : **ë°ì´í„°**ë¡œë¶€í„° ê·œì¹™ì„ í•™ìŠµí•˜ëŠ” AIì˜ í•˜ìœ„ ë¶„ì•¼

DL(Deep Learning) : **Neural network**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ MLì˜ í•˜ìœ„ë¶„ì•¼

![Untitled](Deep%20Learning%20Basic%20875396a2b4f54f888e3f8204ee52cccf/Untitled.png)

## Deep Learning Component

### 1) Data

- ë‹¤ë£¨ëŠ” Taskì— dependent (Classification, Semantic Segmentation, Object Detection, Pose Estimation)

### 2) Model

- inputì—ì„œ featureë¥¼ ë½‘ê³  ìš°ë¦¬ê°€ ì›í•˜ëŠ” outputìœ¼ë¡œ ë§Œë“œëŠ” í”„ë¡œê·¸ë¨

### 3) Loss function

- í•™ìŠµ ì¤‘ ì•Œê³ ë¦¬ì¦˜ì´ ì–¼ë§ˆë‚˜ ì˜ëª» ì˜ˆì¸¡í•˜ëŠ”ê°€ì— ëŒ€í•œ ì§€í‘œ
- ì•Œê³ ë¦¬ì¦˜ì´ ì˜ˆì¸¡í•œ ê°’ê³¼ ì‹¤ì œ ì •ë‹µì˜ ì°¨ì´ë¥¼ ë¹„êµí•˜ì—¬ í•™ìŠµ
- ë‹¤ë£¨ëŠ” Taskì— dependent (MSE, Cross Entropy, MLE, â€¦)

### 4) Optimization and Regularization

Optimization

- Gradient Descent Method(ê²½ì‚¬í•˜ê°•ë²•)
    - loss functionì„ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ì¤„ì´ê¸° ìœ„í•œ ìµœì í™” ê¸°ë²•

Regularization

- í•™ìŠµì„ ë°©í•´í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ â†‘ (ì—¬ëŸ¬ í•™ìŠµ ë°ì´í„°ì—ì„œë„ ì˜ ë™ì‘í•˜ë„ë¡ )

## Neural Network

: **Function Approximators** that stack affine transformations followed by **nonlinear** transformations

## Nonlinear Function

- Activation function(í™œì„±í•¨ìˆ˜)ë¡œ ë¹„ì„ í˜•í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•¨

## Multi-Layer Perceptron

![Untitled](Deep%20Learning%20Basic%20875396a2b4f54f888e3f8204ee52cccf/Untitled%201.png)

## Generalization

- ì¼ë°˜í™” ì„±ëŠ¥
- Generalization Gap = | Test error - Training error |

![í•™ìŠµì„ ë°˜ë³µí• ìˆ˜ë¡ training errorëŠ” ê°ì†Œí•˜ì§€ë§Œ 0ì´ ëë‹¤ê³  í•´ë„ ìµœì ì˜ ê°’ì€ ì•„ë‹˜(ì¼ì • êµ¬ê°„ì´í›„ë¶€í„°ëŠ” test errorê°€ ì¦ê°€í•¨)](Deep%20Learning%20Basic%20875396a2b4f54f888e3f8204ee52cccf/Untitled%202.png)

í•™ìŠµì„ ë°˜ë³µí• ìˆ˜ë¡ training errorëŠ” ê°ì†Œí•˜ì§€ë§Œ 0ì´ ëë‹¤ê³  í•´ë„ ìµœì ì˜ ê°’ì€ ì•„ë‹˜(ì¼ì • êµ¬ê°„ì´í›„ë¶€í„°ëŠ” test errorê°€ ì¦ê°€í•¨)

- **Under-fitting** : í•™ìŠµë°ì´í„°ì—ì„œì¡°ì°¨ ì˜ ë™ì‘í•˜ì§€ ì•ŠëŠ” ê²½ìš°
- **Over-fitting(ê³¼ì í•©)** : í•™ìŠµë°ì´í„°ì—ì„œëŠ” ì˜ ë™ì‘í•˜ì§€ë§Œ í…ŒìŠ¤íŠ¸ë°ì´í„°ì—ì„œëŠ” ì˜ ë™ì‘í•˜ì§€ ì•ŠëŠ” ê²½ìš°

![Untitled](Deep%20Learning%20Basic%20875396a2b4f54f888e3f8204ee52cccf/Untitled%203.png)

- **Cross Validation (êµì°¨ê²€ì¦)**
    - train dataì¤‘ valid dataë¥¼ ì¶”ì¶œí•´ í•™ìŠµ í™•ì¸ ì§€í‘œë¡œ ì‚¬ìš©
- **Ensemble** - ì—¬ëŸ¬ ë¶„ë¥˜ ëª¨ë¸ì„ ì¡°í•©í•´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìœ ë„
    - **Bagging** : data setì„ subsetìœ¼ë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµ í›„ ê°ê°ì˜ votingì´ë‚˜ averagingì„ êµ¬í•¨ (ë³‘ë ¬ í•™ìŠµ)
    - **Boosting** : í•™ìŠµì´ ì œëŒ€ë¡œ ë˜ì§€ ì•Šì€ ë°ì´í„°ë“¤ì„ ëª¨ì•„ ìƒˆë¡œìš´ ê°„ë‹¨í•œ ëª¨ë¸ë¡œ ì¬í•™ìŠµ (ìˆœì°¨ì  í•™ìŠµ)
    
    ![Untitled](Deep%20Learning%20Basic%20875396a2b4f54f888e3f8204ee52cccf/Untitled%204.png)
    
- **Regularization** - í•™ìŠµì„ ë°©í•´
    - **early stopping** - over-fittingì¸ ê²½ìš°
    - **parameter norm penalty**
    - **data argumentation** - ì´ë¯¸ì§€ íšŒì „/ë°ê¸°ì¡°ì ˆ/í¬ë¡­/labelìˆ˜ì • ë“± í•œì •ëœ dataë¡œ ë§ì€ í•™ìŠµ ê°€ëŠ¥
    - **noise robustness** - ì´ìƒì¹˜ë‚˜ ë…¸ì´ì¦ˆê°€ ë“¤ì–´ì™€ë„ í¬ê²Œ í”ë“¤ë¦¬ì§€ ì•ŠìŒ
    - **dropout** - ì„ì˜ì˜ ë…¸ë“œë¥¼ ì¼ì • í™•ë¥ ë¡œ dropí•¨ (í•™ìŠµì— ì°¸ì—¬ì‹œí‚¤ì§€ ì•ŠìŒ)
    - **label smoothing** - ëª¨ë¸ì´ ë„ˆë¬´ í™•ì‹ ì„ ê°€ì§€ì§€ ì•Šë„ë¡(0ê³¼ 1ì´ ì•„ë‹ˆë¼) ë„ì™€ì£¼ì–´ ê³¼ì í•©ì„ ì¤„ì„

## Convolutional Neural Networks (í•©ì„±ê³± ì‹ ê²½ë§)

Fully connected multi layered Neural Networkd(FNN)ì˜ ë¬¸ì œì 

- ì¸ì ‘ í”½ì…€ê°„ì˜ ìƒê´€ê´€ê³„ê°€ ë¬´ì‹œë˜ì–´ ì´ë¯¸ì§€ë¥¼ ë²¡í„°í™”í•˜ëŠ” ê³¼ì •ì—ì„œ ì •ë³´ì†ì‹¤

â†’ CNNìœ¼ë¡œ í•´ê²°

**ì´ë¯¸ì§€ì˜ ê³µê°„ì •ë³´ë¥¼ ìœ ì§€í•˜ë©° í•™ìŠµ** vs ì´ë¯¸ì§€ì˜ ê³µê°„ì •ë³´ë¥¼ ìœ ì§€í•˜ì§€ ì•Šìœ¼ë©° í•™ìŠµ

![Untitled](Deep%20Learning%20Basic%20875396a2b4f54f888e3f8204ee52cccf/Untitled%205.png)

<aside>
ğŸ‘€ {Conv â†’ Activation funcion â†’ pooling} x n â†’ Fully connected layer

</aside>

Convolution ê³„ì‚° ë°©ë²•

- inputê°’(I)ê³¼ í•„í„° ì—­í• ì˜ í–‰ë ¬(K) ë‘ ê°œ í•„ìš”

![Untitled](Deep%20Learning%20Basic%20875396a2b4f54f888e3f8204ee52cccf/Untitled%206.png)

Pooling ê³„ì‚° ë°©ë²•

- íŒŒë¼ë¯¸í„° ìˆ˜ì™€ ì—°ì‚°ëŸ‰ì„ ì¤„ì´ê¸°ìœ„í•œ ë‹¤ìš´ì‚¬ì´ì§•
- max-pooling, average-pooling

## 1x1 Convolution

- depth ì°¨ì› ë³€ê²½ ê°€ëŠ¥ â†’ neural networkë¥¼ ê¹Šê²Œ ìŒ“ì„ ìˆ˜ ìˆìŒ

## Modern CNN

1) AlexNet

- ë‘ ê°œì˜ ë„¤íŠ¸ì›Œí¬
- 11x11 filter
- 5ê°œì˜ convolution layer, 3ê°œì˜ dense layer

2) VGGNet

- 3x3 convolution filer

3) GoogLeNet

- 1x1 convolution

4) ResNet

- ì‚¬ëŒì˜ ëŠ¥ë ¥ì„ ë›°ì–´ë„˜ì€ ì²« ë²ˆì§¸ ëª¨ë¸
- gradient vanishing problemì„ skip connectionì„ ë„ì…í•´ í•´ê²°